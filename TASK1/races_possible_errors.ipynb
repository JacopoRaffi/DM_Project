{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08fb6cb2-9b07-4945-a84b-75355f0fd813",
   "metadata": {},
   "source": [
    "# Checks of races data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cc0af2-1d5c-421f-b0dd-4050e020dc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoreload allows the notebook to dynamically load code: if we update some helper functions *outside* of the notebook, we do not need to reload the notebook.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "50c53be9-36ee-41e1-bc44-18e4dcb05184",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T13:19:47.624138Z",
     "start_time": "2024-09-20T13:19:47.621981Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710db3ad",
   "metadata": {},
   "source": [
    "We load the dataset from a CSV file and display the first few rows to get an initial understanding of the data. This helps us verify that the data has been loaded correctly and gives us a glimpse of its structure and contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2034dd0-9c32-4725-add2-442b89ccebab",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = \"../data/races.csv\"\n",
    "dataset = pd.read_csv(csv_file)\n",
    "dataset.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1cc612",
   "metadata": {},
   "source": [
    "Create a dataset without the personal information of the cyclists, taking only one row per race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0457c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete 'position', 'cyclist', 'cyclist_age', 'cyclist_team' and 'delta' columns\n",
    "races_info = dataset.drop(columns=['position', 'cyclist', 'cyclist_age', 'cyclist_team', 'delta'])\n",
    "\n",
    "# For each row in 'races_info', take only the year-month-day part of 'date' (delete the time)\n",
    "races_info['date'] = races_info['date'].str.split(' ').str[0]\n",
    "\n",
    "# Eliminate duplicates\n",
    "races_info = races_info.drop_duplicates()\n",
    "\n",
    "# Display the first rows of the dataset\n",
    "races_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357c9161",
   "metadata": {},
   "source": [
    "Create dataset from the union of the cyclists and the races data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efa2ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create union of two datasets, merging them considering the url of the cyclist\n",
    "dataset_cyclists = pd.read_csv(\"../data/cyclists.csv\")\n",
    "dataset_races = pd.read_csv(\"../data/races.csv\")\n",
    "merged_dataset = pd.merge(dataset_cyclists, dataset_races, left_on='_url', right_on='cyclist', how='inner')\n",
    "\n",
    "# Modify name column of the cyclist url in '_url_cyclist', and name column of the race url in '_url_race'\n",
    "merged_dataset = merged_dataset.rename(columns={'_url_x': '_url_cyclist', '_url_y': '_url_race'})\n",
    "# Modify name column of the cyclist name in 'name_cyclist', and name column of the race name in 'name_race'\n",
    "merged_dataset = merged_dataset.rename(columns={'name_x': 'name_cyclist', 'name_y': 'name_race'})\n",
    "# Take only the year-month-day part of 'date' (delete the time)\n",
    "merged_dataset['date'] = merged_dataset['date'].str.split(' ').str[0]\n",
    "\n",
    "merged_dataset.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65268c54",
   "metadata": {},
   "source": [
    "## Initial Info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ed1dd8",
   "metadata": {},
   "source": [
    "Now we provide a concise summary of the DataFrame, including the number of non-null entries, data types of each column, and memory usage. It helps us quickly identify missing values and understand the overall structure of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0f08a7-1c79-4080-ac5d-36bb9c6d335b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bc4b75",
   "metadata": {},
   "source": [
    "Also, we generates a descriptive statistics for numerical columns in the DataFrame. It includes metrics such as count, mean, standard deviation, minimum, and maximum values, as well as the 25th, 50th, and 75th percentiles. This summary helps us understand the distribution and central tendency of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1781782",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfbfdfd",
   "metadata": {},
   "source": [
    "## Check on '_url' data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca61a79",
   "metadata": {},
   "source": [
    "We start considering the `_url` column, and check the number of null values and the count the occurrences of each unique value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fed8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of null values in _url column: ' + str(dataset['_url'].isnull().sum())\n",
    "      + ' (' + str(round(dataset['_url'].isnull().sum() / len(dataset) * 100, 2)) + '%)')\n",
    "\n",
    "print('\\nCount occurrences of each value in _url column:')\n",
    "url_counts = dataset['_url'].value_counts()\n",
    "print(url_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f972eba6",
   "metadata": {},
   "source": [
    "We have lots of different values, but no null values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bff8ba",
   "metadata": {},
   "source": [
    "In this block we check if there are `_url` values that are not in the form name/year/stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c82c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split url by / in name, year and stage\n",
    "url_split = dataset['_url'].str.split('/', expand=True)\n",
    "# Check null elements in url_split[0], url_split[1] and url_split[2], and if url_split[1] contains only digits\n",
    "invalid_rows = dataset[url_split[0].isnull() | url_split[1].isnull() | url_split[2].isnull() | ~url_split[1].str.isdigit()]\n",
    "print('Number of invalid URLs: ' + str(len(invalid_rows)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc71338c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Chiedere sulle gare dove ci sono un solo o pochi partecipanti\n",
    "\n",
    "url_counts = dataset['_url'].value_counts()\n",
    "\n",
    "# From url_counts, get the urls where the number of occurrence is less than 2\n",
    "print(url_counts[url_counts < 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75119b38",
   "metadata": {},
   "source": [
    "## Check on 'name' data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fae7ec6",
   "metadata": {},
   "source": [
    "Now we consider the `name` column, and check the number of null values and the count the occurrences of each unique value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f006b928",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of null values in name column: ' + str(dataset['name'].isnull().sum())\n",
    "      + ' (' + str(round(dataset['name'].isnull().sum() / len(dataset) * 100, 2)) + '%)')\n",
    "\n",
    "print('\\n\\nCount occurrences of each value in name column:')\n",
    "name_counts = dataset['name'].value_counts()\n",
    "print(name_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56404ff",
   "metadata": {},
   "source": [
    "We have different values, but no null values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15613535",
   "metadata": {},
   "source": [
    "Since it's small, we print all the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a493cc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all the names that appear, alphabetically ordered\n",
    "print(name_counts.index.sort_values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea97523",
   "metadata": {},
   "source": [
    "For each url, check if all the `name` values are the same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0056629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by '_url' and calculate the number of unique values in the 'name' column\n",
    "name_uniques = dataset.groupby('_url')['name'].nunique()\n",
    "# Filter the URLs with more than one unique name\n",
    "multiple_names_urls = name_uniques[name_uniques > 1].index\n",
    "\n",
    "print('Number of URLs with more than one unique name: ' + str(len(multiple_names_urls)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c54314",
   "metadata": {},
   "source": [
    "In this block we are checking if there are `name` values that contains any incorrect numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a712ee64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rows wehere 'name' value contains any number, except for names containing 'E3' (there are some races with E3 in the name)\n",
    "invalid_rows = dataset[dataset['name'].str.contains(r'\\d') & ~dataset['name'].str.contains('E3')]\n",
    "print('Number of invalid names: ' + str(len(invalid_rows)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce7476c",
   "metadata": {},
   "source": [
    "## Check on 'points' data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddbf1bc",
   "metadata": {},
   "source": [
    "Now we consider the `point` column, and check the number of null values and the count the occurrences of each unique value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be621bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of null values in points column: ' + str(dataset['points'].isnull().sum())\n",
    "      + ' (' + str(round(dataset['points'].isnull().sum() / len(dataset) * 100, 2)) + '%)')\n",
    "\n",
    "print('\\nCount occurrences of each value in points column:')\n",
    "point_counts = dataset['points'].value_counts()\n",
    "print(point_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faa4c21",
   "metadata": {},
   "source": [
    "We have not a lot of values, and few null values. Also, we see that every value is sintatically correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ee4b13",
   "metadata": {},
   "source": [
    "For each url, check if all the `name` values are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6611686c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by '_url' and calculate the number of unique values in the 'points' column\n",
    "points_uniques = dataset.groupby('_url')['points'].nunique()\n",
    "# Filter the URLs with more than one unique points\n",
    "multiple_points_urls = points_uniques[points_uniques > 1].index\n",
    "\n",
    "print('Number of URLs with more than one unique points: ' + str(len(multiple_points_urls)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca65180f",
   "metadata": {},
   "source": [
    "We check the urls where `points` is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd9fb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique data based on '_url' and 'points'\n",
    "unique_data = dataset.drop_duplicates(subset=['_url', 'points'])\n",
    "# Get rows where 'points' is null\n",
    "rows = unique_data[unique_data['points'].isnull()]\n",
    "\n",
    "print('Number of rows with null points: ' + str(len(rows)))\n",
    "print('\\nURLs of the rows with null points:')\n",
    "print(rows['_url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faec7f95",
   "metadata": {},
   "source": [
    "## Check on 'uci_points' data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d95e0b8",
   "metadata": {},
   "source": [
    "Now we consider the `uci_points` column, and check the number of null values and the count the occurrences of each unique value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4ca842",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of null values in uci_points column: ' + str(dataset['uci_points'].isnull().sum())\n",
    "      + ' (' + str(round(dataset['uci_points'].isnull().sum() / len(dataset) * 100, 2)) + '%)')\n",
    "\n",
    "print('\\nCount occurrences of each value in uci_points column:')\n",
    "uci_point_counts = dataset['uci_points'].value_counts()\n",
    "print(uci_point_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bf4e8a",
   "metadata": {},
   "source": [
    "We have different values, but a lot of null values. Also, we see that every value is sintatically correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cb14e2",
   "metadata": {},
   "source": [
    "For each url, check if all the `uci_points` values are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e03d80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by '_url' and calculate the number of unique values in the 'uci_points' column\n",
    "uci_points_uniques = dataset.groupby('_url')['uci_points'].nunique()\n",
    "# Filter the URLs with more than one unique uci_points\n",
    "multiple_uci_points_urls = uci_points_uniques[uci_points_uniques > 1].index\n",
    "\n",
    "print('Number of URLs with more than one unique uci_points: ' + str(len(multiple_uci_points_urls)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7add0369",
   "metadata": {},
   "source": [
    "We check the urls where `uci_points` is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c90effd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique data based on '_url' and 'uci_points'\n",
    "unique_data = dataset.drop_duplicates(subset=['_url', 'uci_points'])\n",
    "# Get rows where 'uci_points' is null\n",
    "rows = unique_data[unique_data['uci_points'].isnull()]\n",
    "\n",
    "print('Number of rows with null uci_points: ' + str(len(rows)))\n",
    "print('\\nURLs of the rows with null uci_points:')\n",
    "print(rows['_url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4be3726",
   "metadata": {},
   "source": [
    "## Check on 'length' data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f3294b",
   "metadata": {},
   "source": [
    "Now we consider the `length` column, and check the number of null values and the count the occurrences of each unique value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5823263",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of null values in length column: ' + str(dataset['length'].isnull().sum())\n",
    "      + ' (' + str(round(dataset['length'].isnull().sum() / len(dataset) * 100, 2)) + '%)')\n",
    "\n",
    "print('\\nCount occurrences of each value in length column:')\n",
    "length_counts = dataset['length'].value_counts()\n",
    "print(length_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc468540",
   "metadata": {},
   "source": [
    "We have a lot of values, but no null values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630444a1",
   "metadata": {},
   "source": [
    "For each url, check if all the `length` values are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597f3ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by '_url' and calculate the number of unique values in the 'length' column\n",
    "length_uniques = dataset.groupby('_url')['length'].nunique()\n",
    "# Filter the URLs with more than one unique length\n",
    "multiple_length_urls = length_uniques[length_uniques > 1].index\n",
    "\n",
    "print('Number of URLs with more than one unique length: ' + str(len(multiple_length_urls)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91efb7fd",
   "metadata": {},
   "source": [
    "Since we have a lot of values, we check if every value is sintatically correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f126f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rows where 'length' is not a digit\n",
    "invalid_rows = dataset[~dataset['length'].astype(str).str.replace('.', '').str.isdigit()]\n",
    "\n",
    "print('Number of invalid lengths: ' + str(len(invalid_rows)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b86d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rows where 'length' does not end with '.0'\n",
    "invalid_rows = dataset[~dataset['length'].astype(str).str.endswith('.0')]\n",
    "                                \n",
    "print('Number of invalid lengths: ' + str(len(invalid_rows)))\n",
    "for index, row in invalid_rows.iterrows():\n",
    "    print(row['_url'], row['length'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5ebd5d",
   "metadata": {},
   "source": [
    "Check the races where the `length` value is small or large, for possible outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05956185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset info, for 'length' column\n",
    "dataset['length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8de384",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2000\n",
    "# Get rows where 'length' is smaller than n\n",
    "filtered_data = races_info[races_info['length'] < n]\n",
    "\n",
    "print('Rows where length is smaller than ' + str(n) + ':')\n",
    "for index, row in filtered_data.iterrows():\n",
    "    # Stampa '_url' e i corrispondenti 'length'\n",
    "    print(index, row['_url'], row['length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1339a98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 300000\n",
    "# Get data where 'length' is greater than n\n",
    "filtered_data = races_info[races_info['length'] > n]\n",
    "\n",
    "print('Rows where length is greater than ' + str(n) + ':')\n",
    "for index, row in filtered_data.iterrows():\n",
    "    # Stampa '_url' e i corrispondenti 'length'\n",
    "    print(index, row['_url'], row['length'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc7c33a",
   "metadata": {},
   "source": [
    "## Check on 'climb_total' data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70f8853",
   "metadata": {},
   "source": [
    "Now we consider the `climb_total` column, and check the number of null values and the count the occurrences of each unique value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d7d21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of null values in climb_total column: ' + str(dataset['climb_total'].isnull().sum())\n",
    "      + ' (' + str(round(dataset['climb_total'].isnull().sum() / len(dataset) * 100, 2)) + '%)')\n",
    "\n",
    "print('\\nCount occurrences of each value in climb_total column:')\n",
    "climb_total_counts = dataset['climb_total'].value_counts()\n",
    "print(climb_total_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1850bc",
   "metadata": {},
   "source": [
    "We have a lot of different values, and a lot of null values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34319da",
   "metadata": {},
   "source": [
    "For each url, check if all the `climb_total` values are the same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63667cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by '_url' and calculate the number of unique values in the 'climb_total' column\n",
    "climb_total_uniques = dataset.groupby('_url')['climb_total'].nunique()\n",
    "# Filter the URLs with more than one unique climb_total\n",
    "multiple_climb_total_urls = climb_total_uniques[climb_total_uniques > 1].index\n",
    "\n",
    "print('Number of URLs with more than one unique climb_total: ' + str(len(multiple_climb_total_urls)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71e93e2",
   "metadata": {},
   "source": [
    "Since we have a lot of values, we check if every value is sintatically correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891df8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rows where 'climb_total' is not a digit\n",
    "invalid_rows = dataset[~dataset['climb_total'].astype(str).str.replace('.', '').str.isdigit()].dropna(subset=['climb_total'])\n",
    "\n",
    "print('Number of invalid climb_total: ' + str(len(invalid_rows)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cfa259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rows where 'climb_total' does not end with '.0'\n",
    "invalid_rows = dataset[~dataset['climb_total'].astype(str).str.endswith('.0')].dropna(subset=['climb_total'])\n",
    "\n",
    "print('Number of invalid climb_total: ' + str(len(invalid_rows)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afcdfd6",
   "metadata": {},
   "source": [
    "Check the races where the `climb_total` value is small or large, for possible outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383b6457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset info, for 'climb_total' column\n",
    "dataset['climb_total'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df88df0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "# Get rows where 'climb_total' is smaller than n\n",
    "filtered_data = races_info[races_info['climb_total'] < n]\n",
    "\n",
    "print('Rows where climb_total is smaller than ' + str(n) + ':')\n",
    "for index, row in filtered_data.iterrows():\n",
    "    # Stampa '_url' e i corrispondenti 'climb_total'\n",
    "    print(index, row['_url'], row['climb_total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eebe83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 6000\n",
    "# Get data where 'climb_total' is greater than n\n",
    "filtered_data = races_info[races_info['climb_total'] > n]\n",
    "\n",
    "print('Rows where climb_total is greater than ' + str(n) + ':')\n",
    "for index, row in filtered_data.iterrows():\n",
    "    # Stampa '_url' e i corrispondenti 'climb_total'\n",
    "    print(index, row['_url'], row['climb_total'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119f32f3",
   "metadata": {},
   "source": [
    "## Check on 'profile' data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f193d3f4",
   "metadata": {},
   "source": [
    "Now we consider the `profile` column, and check the number of null values and the count the occurrences of each unique value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a55c560",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of null values in profile column: ' + str(dataset['profile'].isnull().sum())\n",
    "      + ' (' + str(round(dataset['profile'].isnull().sum() / len(dataset) * 100, 2)) + '%)')\n",
    "\n",
    "print('\\nCount occurrences of each value in profile column:')\n",
    "profile_counts = dataset['profile'].value_counts()\n",
    "print(profile_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851c01a6",
   "metadata": {},
   "source": [
    "We have few different values, but a lot of null values. Also, we see that every value is sintatically correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1847c6a",
   "metadata": {},
   "source": [
    "For each url, check if all the `profile` values are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da929a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by '_url' and calculate the number of unique values in the 'profile' column\n",
    "profile_uniques = dataset.groupby('_url')['profile'].nunique()\n",
    "# Filter the URLs with more than one unique profile\n",
    "multiple_profile_urls = profile_uniques[profile_uniques > 1].index\n",
    "\n",
    "print('Number of URLs with more than one unique profile: ' + str(len(multiple_profile_urls)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377a3211",
   "metadata": {},
   "source": [
    "## Check on 'startlist_quality' data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87363e49",
   "metadata": {},
   "source": [
    "Now we consider the `startlist_quality` column, and check the number of null values and the count the occurrences of each unique value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b55f713",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of null values in startlist_quality column: ' + str(dataset['startlist_quality'].isnull().sum())\n",
    "      + ' (' + str(round(dataset['startlist_quality'].isnull().sum() / len(dataset) * 100, 2)) + '%)')\n",
    "\n",
    "print('\\nCount occurrences of each value in startlist_quality column:')\n",
    "startlist_quality_counts = dataset['startlist_quality'].value_counts()\n",
    "print(startlist_quality_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15837b1f",
   "metadata": {},
   "source": [
    "We have different values, but no null values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c824c683",
   "metadata": {},
   "source": [
    "For each url, check if all the `startlist_quality` values are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba336068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by '_url' and calculate the number of unique values in the 'startlist_quality' column\n",
    "startlist_quality_uniques = dataset.groupby('_url')['startlist_quality'].nunique()\n",
    "# Filter the URLs with more than one unique startlist_quality\n",
    "multiple_startlist_quality_urls = startlist_quality_uniques[startlist_quality_uniques > 1].index\n",
    "\n",
    "print('Number of URLs with more than one unique startlist_quality: ' + str(len(multiple_startlist_quality_urls)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f9f3a8",
   "metadata": {},
   "source": [
    "Since we have a lot of different values, we check if every value is sintatically correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7ac9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rows where 'startlist_quality' is not a digit\n",
    "invalid_rows = dataset[~dataset['startlist_quality'].astype(str).str.replace('.', '').str.isdigit()]\n",
    "\n",
    "print('Number of invalid startlist_quality: ' + str(len(invalid_rows))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bf1bda",
   "metadata": {},
   "source": [
    "## Check on 'average_temperature' data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f4a284",
   "metadata": {},
   "source": [
    "Now we consider the `average_temperature` column, and check the number of null values and the count the occurrences of each unique value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d07f200",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of null values in average_temperature column: ' + str(dataset['average_temperature'].isnull().sum())\n",
    "      + ' (' + str(round(dataset['average_temperature'].isnull().sum() / len(dataset) * 100, 2)) + '%)')\n",
    "\n",
    "print('\\nCount occurrences of each value in average_temperature column:')\n",
    "average_temperature_counts = dataset['average_temperature'].value_counts()\n",
    "print(average_temperature_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a1de22",
   "metadata": {},
   "source": [
    "We have different values, and almost all the values are null. Also, we cas see that every value is sintatically correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d339b43",
   "metadata": {},
   "source": [
    "For each url, check if all the `average_temperature` values are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4285125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by '_url' and calculate the number of unique values in the 'average_temperature' column\n",
    "average_temperature_uniques = dataset.groupby('_url')['average_temperature'].nunique()\n",
    "# Filter the URLs with more than one unique average_temperature\n",
    "multiple_average_temperature_urls = average_temperature_uniques[average_temperature_uniques > 1].index\n",
    "\n",
    "print('Number of URLs with more than one unique average_temperature: ' + str(len(multiple_average_temperature_urls)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40153293",
   "metadata": {},
   "source": [
    "## Check on 'date' data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9b91d6",
   "metadata": {},
   "source": [
    "Now we consider the `date` column, and check the number of null values and the count the occurrences of each unique value. We do this considerig the merged dataset where we don't have time value of the date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577ade08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of null values in date column: ' + str(merged_dataset['date'].isnull().sum())\n",
    "      + ' (' + str(round(merged_dataset['date'].isnull().sum() / len(merged_dataset) * 100, 2)) + '%)')\n",
    "\n",
    "print('\\nCount occurrences of each value in date column:')\n",
    "date_counts = merged_dataset['date'].value_counts()\n",
    "print(date_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7cbba5",
   "metadata": {},
   "source": [
    "We have different values, but no null values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a60a643",
   "metadata": {},
   "source": [
    "For each url, check if all the `date` values are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9518978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by '_url' and calculate the number of unique values in the 'date' column\n",
    "date_uniques = dataset.groupby('_url')['date'].nunique()\n",
    "# Filter the URLs with more than one unique date\n",
    "multiple_date_urls = date_uniques[date_uniques > 1].index\n",
    "\n",
    "print('Number of URLs with more than one unique date: ' + str(len(multiple_date_urls)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0eb853",
   "metadata": {},
   "source": [
    "Since we have a lot of different values, we check if every value is sintatically correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23765deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are any 'date' values in the format yyyy-mm-dd hh:mm:ss (in the races dataset)\n",
    "invalid_rows = dataset[~dataset['date'].str.match(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}')]\n",
    "print('Number of invalid dates (format yyyy-mm-dd hh:mm:ss, in the races dataset): ' + str(len(invalid_rows)))\n",
    "\n",
    "# Check if there are any 'date' values not in the format yyyy-mm-dd (in the merged dataset)\n",
    "invalid_rows = merged_dataset[~merged_dataset['date'].str.match(r'\\d{4}-\\d{2}-\\d{2}')]\n",
    "print('Number of invalid dates (format yyyy-mm-dd, in the merged dataset): ' + str(len(invalid_rows)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988ee472",
   "metadata": {},
   "source": [
    "Check if the year is the same in both the `_url` and the `date`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b0b475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split _url by / into name, year, and stage\n",
    "url_split = dataset['_url'].str.split('/', expand=True) # expand=True to return a DataFrame\n",
    "# Extract the year from the date column (assuming format yyyy-mm-dd hh:mm:ss)\n",
    "date_year = dataset['date'].str[:4]\n",
    "# Compare the year in the _url (from the second part of the split) with the year in the date\n",
    "mismatched_years = dataset[(url_split[1] != date_year)]\n",
    "\n",
    "# Print the number of rows where the year does not match\n",
    "print(f\"Number of rows where the year in the url does not match the year in the date: {len(mismatched_years)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0198f47c",
   "metadata": {},
   "source": [
    "## Check on 'position' data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1421a63",
   "metadata": {},
   "source": [
    "Now we consider the `position` column, and check the number of null values and the count the occurrences of each unique value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e459e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of null values in position column: ' + str(dataset['position'].isnull().sum())\n",
    "      + ' (' + str(round(dataset['position'].isnull().sum() / len(dataset) * 100, 2)) + '%)')\n",
    "\n",
    "print('\\nCount occurrences of each value in position column:')\n",
    "position_counts = dataset['position'].value_counts()\n",
    "print(position_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3b47af",
   "metadata": {},
   "source": [
    "We have different values, and no null values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75271c14",
   "metadata": {},
   "source": [
    "Since we have different values, we check if every value is sintatically correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bff1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are any 'position' value that is not a digit\n",
    "invalid_rows = dataset[~dataset['position'].astype(str).str.replace('.', '').str.isdigit()]\n",
    "\n",
    "print('Number of invalid positions: ' + str(len(invalid_rows)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b970a50",
   "metadata": {},
   "source": [
    "For each url, check if there are all the `position` values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa66f314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if the positions are from 0 to the max one after the other\n",
    "def check_positions(positions):\n",
    "    return np.array_equal(np.sort(positions), np.arange(positions.max() + 1))\n",
    "\n",
    "# Apply the function to the dataset\n",
    "invalid_urls = dataset.groupby('_url')['position'].apply(lambda x: not check_positions(x))\n",
    "\n",
    "# Stampa gli '_url' che non rispettano la condizione\n",
    "print('Number of URLs with invalid positions: ' + str(len(invalid_urls[invalid_urls])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6531cf57",
   "metadata": {},
   "source": [
    "## Check on 'cyclist' data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd076a3",
   "metadata": {},
   "source": [
    "Now we consider the `cyclist` column, and check the number of null values and the count the occurrences of each unique value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a23127",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of null values in cyclist column: ' + str(dataset['cyclist'].isnull().sum())\n",
    "      + ' (' + str(round(dataset['cyclist'].isnull().sum() / len(dataset) * 100, 2)) + '%)')\n",
    "\n",
    "print('\\nCount occurrences of each value in cyclist column:')\n",
    "cyclist_counts = dataset['cyclist'].value_counts()\n",
    "print(cyclist_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c78d397",
   "metadata": {},
   "source": [
    "We have lots of different values, but no null values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfa144b",
   "metadata": {},
   "source": [
    "We check if all the cyclists are different in the same race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251a6e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each url, check if the a cyclist appears more than once\n",
    "url_cyclist_count = dataset.groupby('_url')['cyclist'].value_counts()\n",
    "invalid_entries = url_cyclist_count[url_cyclist_count > 1]\n",
    "\n",
    "print('Number of URLs with a cyclist appearing more than once: ' + str(len(invalid_entries.index.get_level_values(0))))\n",
    "\n",
    "# Estrai gli _url e i ciclisti che compaiono più volte\n",
    "for (url, cyclist), count in invalid_entries.items():\n",
    "    print(f\"URL: {url}, Cyclist: {cyclist}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1087f0c1",
   "metadata": {},
   "source": [
    "Considering the two datasets, we check if all the cyclists in cyclists.csv are in races.csv, and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120805ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are any 'cyclist' values in the races dataset that are not in the cyclists dataset\n",
    "invalid_rows = dataset[~dataset['cyclist'].isin(dataset_cyclists['_url'])]\n",
    "\n",
    "print('Number of cyclists with no info: ' + str(len(invalid_rows)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d79a551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are any '_url' values in the cyclists dataset that are not in the races dataset\n",
    "invalid_rows = dataset_cyclists[~dataset_cyclists['_url'].isin(dataset['cyclist'])]\n",
    "\n",
    "print('Number of cyclists that are not in any race: ' + str(len(invalid_rows)))\n",
    "for index, row in invalid_rows.iterrows():\n",
    "    print(row['_url'], row['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fc8594",
   "metadata": {},
   "source": [
    "## Check on 'cyclist_age' data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79319d77",
   "metadata": {},
   "source": [
    "Now we consider the `cyclist_age` column, and check the number of null values and the count the occurrences of each unique value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08092e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of null values in cyclist_age column: ' + str(dataset['cyclist_age'].isnull().sum())\n",
    "      + ' (' + str(round(dataset['cyclist_age'].isnull().sum() / dataset.shape[0] * 100, 2)) + '%)')\n",
    "\n",
    "print('\\nCount occurrences of each value in cyclist_age column:')\n",
    "cyclist_age_counts = dataset['cyclist_age'].value_counts()\n",
    "print(cyclist_age_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e4df8b",
   "metadata": {},
   "source": [
    "We have different values, and just a few of null values. Also, we see that every value is sintatically correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd53cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each data, print '_url' where 'cyclist_age' is NaN\n",
    "for index, row in dataset[dataset['cyclist_age'].isnull()].iterrows():\n",
    "    print(row['_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11bc9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are any 'cyclist_age' null values, where we have the year of birth in the cyclists dataset\n",
    "invalid_rows = merged_dataset[merged_dataset['cyclist_age'].isnull() & merged_dataset['birth_year'].notnull()]\n",
    "\n",
    "print('Number of cyclists with age info in cyclists dataset: ' + str(len(invalid_rows)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65566d2",
   "metadata": {},
   "source": [
    "Check the races where the `cyclist_age` value is small or large, for possible outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ec37de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset info, for 'cyclist_age' column\n",
    "dataset['cyclist_age'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d352afad",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 18\n",
    "# Get rows where 'cyclist_age' is smaller than n\n",
    "filtered_data = dataset[dataset['cyclist_age'] < n]\n",
    "\n",
    "print('Rows where cyclist age is smaller than ' + str(n) + ':')\n",
    "for index, row in filtered_data.iterrows():\n",
    "    # Stampa '_url' e i corrispondenti 'cyclist_age'\n",
    "    print(index, row['_url'], row['cyclist_age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714af727",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50\n",
    "# Get data where 'cyclist_age' is greater than n\n",
    "filtered_data = dataset[dataset['cyclist_age'] > n]\n",
    "\n",
    "print('Rows where cyclist_age is greater than ' + str(n) + ':')\n",
    "for index, row in filtered_data.iterrows():\n",
    "    # Stampa '_url' e i corrispondenti 'cyclist_age'\n",
    "    print(index, row['_url'], row['cyclist_age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69518fc6",
   "metadata": {},
   "source": [
    "## Check on 'is_tarmac' data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab07bf4",
   "metadata": {},
   "source": [
    "Now we consider the `is_tarmac` column, and check the number of null values and the count the occurrences of each unique value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7301d89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of null values in is_tarmac column: ' + str(dataset['is_tarmac'].isnull().sum())\n",
    "      + ' (' + str(round(dataset['is_tarmac'].isnull().sum() / len(dataset) * 100, 2)) + '%)')\n",
    "\n",
    "print('\\nCount occurrences of each value in is_tarmac column:')\n",
    "is_tarmac_counts = dataset['is_tarmac'].value_counts()\n",
    "print(is_tarmac_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7080b0",
   "metadata": {},
   "source": [
    "We have two different values, and no null values. Also, we see that every value is sintatically correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405bbe90",
   "metadata": {},
   "source": [
    "For each url, check if all the `is_tarmac` values are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e8cde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by '_url' and calculate the number of unique values in the 'is_tarmac' column\n",
    "is_tarmac_uniques = dataset.groupby('_url')['is_tarmac'].nunique()\n",
    "# Filter the URLs with more than one unique is_tarmac\n",
    "multiple_is_tarmac_urls = is_tarmac_uniques[is_tarmac_uniques > 1].index\n",
    "\n",
    "print('Number of URLs with more than one unique is_tarmac: ' + str(len(multiple_is_tarmac_urls)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6616a81",
   "metadata": {},
   "source": [
    "## Check on 'is_cobbled' data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7aa31b",
   "metadata": {},
   "source": [
    "Now we consider the `is_cobbled` column, and check the number of null values and the count the occurrences of each unique value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3c8df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of null values in is_cobbled column: ' + str(dataset['is_cobbled'].isnull().sum())\n",
    "      + ' (' + str(round(dataset['is_cobbled'].isnull().sum() / dataset.shape[0] * 100, 2)) + '%)')\n",
    "\n",
    "print('\\nCount occurrences of each value in is_cobbled column:')\n",
    "is_cobbled_counts = dataset['is_cobbled'].value_counts()\n",
    "print(is_cobbled_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465ba4b9",
   "metadata": {},
   "source": [
    "We have one value, and no null values. Also, we see that the value is sintatically correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1094c1e1",
   "metadata": {},
   "source": [
    "## Check on 'is_gravel' data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb26a4ba",
   "metadata": {},
   "source": [
    "Now we consider the `is_gravel` column, and check the number of null values and the count the occurrences of each unique value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234d6d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of null values in is_gravel column: ' + str(dataset['is_gravel'].isnull().sum())\n",
    "      + ' (' + str(round(dataset['is_gravel'].isnull().sum() / dataset.shape[0] * 100, 2)) + '%)')\n",
    "\n",
    "print('\\nCount occurrences of each value in is_gravel column:')\n",
    "is_gravel_counts = dataset['is_gravel'].value_counts()\n",
    "print(is_gravel_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683d715f",
   "metadata": {},
   "source": [
    "We have one value, and no null values. Also, we see that the value is sintatically correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699f34ec",
   "metadata": {},
   "source": [
    "## Check on 'cyclist_team' data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a764a15",
   "metadata": {},
   "source": [
    "Now we consider the `cyclist_team` column, and check the number of null values and the count the occurrences of each unique value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "f73bb024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of null values in cyclist_team column: 159161 (26.98%)\n",
      "\n",
      "Count occurrences of each value in cyclist_team column:\n",
      "cyclist_team\n",
      "liberty-seguros-wurth-team-2005     8869\n",
      "roompot-nederlandse-loterij-2018    8773\n",
      "chazal-vetta-mbk-1993               8094\n",
      "kondor-1979                         7895\n",
      "kazakhstan-2019                     7701\n",
      "                                    ... \n",
      "atala-ofmega-1988                   1259\n",
      "finland-2016                        1236\n",
      "south-africa-1993                   1174\n",
      "denmark-2003                         216\n",
      "quickstep-innergetic-2009              3\n",
      "Name: count, Length: 91, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print total number of null values in 'delta' column, and the percentage of null values (float with two decimal digits after the comma)\n",
    "print('Total number of null values in cyclist_team column: ' + str(dataset['cyclist_team'].isnull().sum())\n",
    "      + ' (' + str(round(dataset['cyclist_team'].isnull().sum() / dataset.shape[0] * 100, 2)) + '%)')\n",
    "\n",
    "print('\\nCount occurrences of each value in cyclist_team column:')\n",
    "cyclist_team_counts = dataset['cyclist_team'].value_counts()\n",
    "print(cyclist_team_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f00498",
   "metadata": {},
   "source": [
    "We have different values, and a lot of null values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748e2ec1",
   "metadata": {},
   "source": [
    "Since we have a lot of different values, we check if every value is sintatically correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1b562608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each data, check if 'cyclist_team' is in the formato team-year, where the last four characters are digits\n",
    "for index, row in dataset.iterrows():\n",
    "    if not pd.isnull(row['cyclist_team']):\n",
    "        if not re.match(r'.+-\\d{4}', row['cyclist_team']):\n",
    "            print(row['_url'], row['cyclist_team'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e6b32c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each data, check if 'cyclist_team' has the last four characters as digits\n",
    "for index, row in dataset.iterrows():\n",
    "    if not pd.isnull(row['cyclist_team']):\n",
    "        if not row['cyclist_team'][-4:].isdigit():\n",
    "            print(row['_url'], row['cyclist_team'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ce8981",
   "metadata": {},
   "source": [
    "Check if the same cyclist is in two cyclist teams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d45891b",
   "metadata": {},
   "source": [
    "## Check on 'delta' data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d4bdc4",
   "metadata": {},
   "source": [
    "Now we consider the `delta` column, and check the number of null values and the count the occurrences of each unique value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56afe1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of null values in delta column: ' + str(dataset['delta'].isnull().sum())\n",
    "        + ' (' + str(round(dataset['delta'].isnull().sum() / dataset.shape[0] * 100, 2)) + '%)')\n",
    "\n",
    "print('\\nCount occurrences of each value in delta column:')\n",
    "delta_counts = dataset['delta'].value_counts()\n",
    "print(delta_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66b6ea9",
   "metadata": {},
   "source": [
    "We have lots of different values, but no null values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2437cf8",
   "metadata": {},
   "source": [
    "Since we have a lot of different values, we check if every value is sintatically correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6baa1266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each data, check if 'delta' has .0 at the end\n",
    "for index, row in dataset.iterrows():\n",
    "    if not pd.isnull(row['delta']):\n",
    "        if not str(row['delta']).endswith('.0'):\n",
    "            print(row['_url'], row['delta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aad90ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each data, check if 'delta' float64 data is a digit\n",
    "for index, row in dataset.iterrows():\n",
    "    if not pd.isnull(row['delta']):\n",
    "        # Delete last two char from 'delta'\n",
    "        delta = str(row['delta'])[:-2]\n",
    "\n",
    "        if not delta.isdigit():\n",
    "            print(row['_url'], row['delta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccb7315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each data, check if 'delta' is less than 0\n",
    "for index, row in dataset.iterrows():\n",
    "    if not pd.isnull(row['delta']):\n",
    "        if row['delta'] < 0:\n",
    "            print(row['_url'], row['delta'])\n",
    "\n",
    "        if not delta.isdigit():\n",
    "            print(row['_url'], row['delta'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631c55b7",
   "metadata": {},
   "source": [
    "Check if following the `positon`order, the delta is ordered too"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
