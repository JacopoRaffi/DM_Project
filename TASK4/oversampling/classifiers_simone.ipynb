{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "### Data Mining Project 2024/25\n",
    "\n",
    "Authors: Nicola Emmolo, Simone Marzeddu, Jacopo Raffi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 11:13:43.581418: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-15 11:13:43.582451: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-15 11:13:43.586216: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-15 11:13:43.597279: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1734257623.611777  359477 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1734257623.616104  359477 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-15 11:13:43.631796: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f153585b250>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arakiwi/dm/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "#to find the best set of parameter setting, we can run a grid search\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import wittgenstein as lw\n",
    "import keras_tuner\n",
    "import keras\n",
    "from keras_tuner import HyperParameters\n",
    "import tensorflow as tf\n",
    "\n",
    "from statistics import mean, stdev\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "from scipy.stats import loguniform as sp_loguniform\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../../data/ml_datasets/oversampling/train_set.csv').sample(frac = 1, random_state=RANDOM_STATE) # shuffling the data so not to introduce bias\n",
    "val_data = pd.read_csv('../../data/ml_datasets/oversampling/val_set.csv')\n",
    "testing_data = pd.read_csv('../../data/ml_datasets/oversampling/test_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = train_data.pop('label')\n",
    "val_label = val_data.pop('label')\n",
    "test_label = testing_data.pop('label')\n",
    "\n",
    "train_set = train_data\n",
    "train_set['race_season%autumn'] = train_set['race_season%autumn'].astype(int)\n",
    "train_set['race_season%spring'] = train_set['race_season%spring'].astype(int)\n",
    "train_set['race_season%summer'] = train_set['race_season%summer'].astype(int)\n",
    "train_set['race_season%winter'] = train_set['race_season%winter'].astype(int)\n",
    "\n",
    "val_set = val_data\n",
    "val_set['race_season%autumn'] = val_set['race_season%autumn'].astype(int)\n",
    "val_set['race_season%spring'] = val_set['race_season%spring'].astype(int)\n",
    "val_set['race_season%summer'] = val_set['race_season%summer'].astype(int)\n",
    "val_set['race_season%winter'] = val_set['race_season%winter'].astype(int)\n",
    "\n",
    "test_set = testing_data\n",
    "test_set['race_season%autumn'] = test_set['race_season%autumn'].astype(int)\n",
    "test_set['race_season%spring'] = test_set['race_season%spring'].astype(int)\n",
    "test_set['race_season%summer'] = test_set['race_season%summer'].astype(int)\n",
    "test_set['race_season%winter'] = test_set['race_season%winter'].astype(int)\n",
    "\n",
    "N_JOBS = 8\n",
    "USER = 'Simone'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FEATURES = len(train_set.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.to_numpy()\n",
    "train_label = train_label.to_numpy()\n",
    "\n",
    "val_set = val_set.to_numpy()\n",
    "val_label = val_label.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = np.concatenate([\n",
    "    np.full(len(train_set), -1),  # -1 per training\n",
    "    np.zeros(len(val_set))   # 0 per validation\n",
    "])\n",
    "\n",
    "X_combined = np.vstack((train_set, val_set))\n",
    "y_combined = np.concatenate((train_label, val_label))\n",
    "\n",
    "ps = PredefinedSplit(test_fold=split_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the parameters' values you want to try\n",
    "def f1_class_scorer(class_index):\n",
    "    def score_function(y_true, y_pred):\n",
    "        # Calcola F1 per ciascuna classe e ritorna quella specificata\n",
    "        return f1_score(y_true, y_pred, average=None)[class_index]\n",
    "    return make_scorer(score_function)\n",
    "\n",
    "# Scorer per la classe 0 e 1\n",
    "f1_class_0 = f1_class_scorer(0)  # Classe 0\n",
    "f1_class_1 = f1_class_scorer(1)  # Classe 1\n",
    "\n",
    "\n",
    "scoring={\n",
    "        'f1_macro': 'f1_macro',   # F1 macro per entrambe le classi\n",
    "        'f1_0': f1_class_0,  # F1 solo per classe 0\n",
    "        'f1_1': f1_class_1   # F1 solo per classe 1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\"max_depth\": [3, 5, 10, 15, 20, None],\n",
    "              \"max_features\": sp_randint(3, N_FEATURES + 1),\n",
    "              \"min_samples_split\": [20, 30, 50, 100],\n",
    "              \"min_samples_leaf\": [10, 20, 30, 50, 100],\n",
    "              \"criterion\": [\"entropy\", \"gini\"],\n",
    "              \"class_weight\":['balanced', None, {0: 0.8, 1: 0.2}, {0: 0.6, 1: 0.4}]} # class weights are related to over/undersampling chosen\n",
    "#define the number of iters\n",
    "n_iter_search = 200 # Total-Iteration: 400\n",
    "#define the model\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "#define the grid search\n",
    "rand_search = RandomizedSearchCV(clf, param_distributions=param_dist, \n",
    "                            n_iter=n_iter_search, \n",
    "                            n_jobs=N_JOBS, \n",
    "                            scoring=scoring,\n",
    "                            refit=False,\n",
    "                            cv=ps)\n",
    "#run the grid search\n",
    "rand_search.fit(X_combined, y_combined);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_f1_macro</th>\n",
       "      <th>mean_test_f1_0</th>\n",
       "      <th>mean_test_f1_1</th>\n",
       "      <th>rank_test_f1_macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0.633910</td>\n",
       "      <td>0.870124</td>\n",
       "      <td>0.397696</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.632964</td>\n",
       "      <td>0.868205</td>\n",
       "      <td>0.397723</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.631756</td>\n",
       "      <td>0.867465</td>\n",
       "      <td>0.396046</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0.631488</td>\n",
       "      <td>0.914229</td>\n",
       "      <td>0.348748</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.631374</td>\n",
       "      <td>0.872311</td>\n",
       "      <td>0.390438</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_test_f1_macro  mean_test_f1_0  mean_test_f1_1  rank_test_f1_macro\n",
       "139            0.633910        0.870124        0.397696                   1\n",
       "61             0.632964        0.868205        0.397723                   2\n",
       "103            0.631756        0.867465        0.396046                   3\n",
       "176            0.631488        0.914229        0.348748                   4\n",
       "111            0.631374        0.872311        0.390438                   5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(rand_search.cv_results_)\n",
    "df.sort_values(by='rank_test_f1_macro', inplace=True)\n",
    "df.to_csv(f'../../data/ml_datasets/oversampling/model_selection/{USER}_decision_tree_results.csv', index=False)\n",
    "df.head()[['mean_test_f1_macro', 'mean_test_f1_0', 'mean_test_f1_1', 'rank_test_f1_macro']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the parameters' values you want to try\n",
    "param_dist = {\"C\": sp_loguniform(1e-4, 1e2)}\n",
    "#define the number of iters\n",
    "n_iter_search = 50 # Total-Iteration: 100\n",
    "#define the model\n",
    "clf = LinearSVC()\n",
    "#define the grid search\n",
    "rand_search = RandomizedSearchCV(clf, param_distributions=param_dist, \n",
    "                            n_iter=n_iter_search, \n",
    "                            n_jobs=N_JOBS, \n",
    "                            scoring=scoring,\n",
    "                            refit=False,\n",
    "                            cv=ps)\n",
    "#run the grid search\n",
    "rand_search.fit(X_combined, y_combined);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rand_search.cv_results_)\n",
    "df.sort_values(by='rank_test_f1_macro', inplace=True)\n",
    "df.to_csv(f'../../data/ml_datasets/oversampling/model_selection/{USER}_svm_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the parameters' values you want to try\n",
    "scoring_metrics = {\n",
    "    'recall': 'recall',\n",
    "    'precision': 'precision',\n",
    "    'f1': 'f1'\n",
    "}\n",
    "\n",
    "param_dist = {}\n",
    "#define the number of iters\n",
    "n_iter_search = 1\n",
    "#define the model\n",
    "clf = GaussianNB()\n",
    "#define the grid search\n",
    "rand_search = RandomizedSearchCV(clf, param_distributions=param_dist, #CrossValidation per confrontabilitÃ , non model selection\n",
    "                            n_iter=n_iter_search, \n",
    "                            n_jobs=1, \n",
    "                            scoring=scoring,\n",
    "                            refit=False,\n",
    "                            cv=ps)\n",
    "#run the grid search\n",
    "rand_search.fit(X_combined, y_combined);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rand_search.cv_results_)\n",
    "df.sort_values(by='rank_test_f1_macro', inplace=True)\n",
    "df.to_csv(f'../../data/ml_datasets/oversampling/model_selection/{USER}_naive_bayes_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rischiamo che il mapping degli attributi categorici ordinali (senza one-hot) crei problemi nel K-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {# 'n_neighbors': [5, 15, 25], # Jacopo\n",
    "              'n_neighbors': [40, 50], # Simone\n",
    "              'algorithm': ['ball_tree', 'kd_tree', 'brute'],}\n",
    "\n",
    "tmp_train_set = train_data.drop(columns=['cyclist_age_group', 'race_season%autumn', 'race_season%spring', 'race_season%summer', 'race_season%winter']).to_numpy()\n",
    "tmp_val_set = val_data.drop(columns=['cyclist_age_group', 'race_season%autumn', 'race_season%spring', 'race_season%summer', 'race_season%winter']).to_numpy()\n",
    "\n",
    "split_index_knn = np.concatenate([\n",
    "    np.full(len(tmp_train_set), -1),  # -1 per training\n",
    "    np.zeros(len(tmp_val_set))   # 0 per validation\n",
    "])\n",
    "\n",
    "X_combined_knn = np.vstack((tmp_train_set, tmp_val_set))\n",
    "y_combined_knn = np.concatenate((train_label, val_label))\n",
    "\n",
    "ps_knn = PredefinedSplit(test_fold=split_index_knn)\n",
    "\n",
    "clf = KNeighborsClassifier()\n",
    "#define the grid search\n",
    "rand_search = GridSearchCV(clf, param_grid=param_dist,\n",
    "                            n_jobs=N_JOBS, \n",
    "                            scoring=scoring,\n",
    "                            refit=False,\n",
    "                            cv=ps_knn)\n",
    "#run the grid search\n",
    "rand_search.fit(X_combined_knn, y_combined_knn);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rand_search.cv_results_)\n",
    "df.sort_values(by='rank_test_f1_macro', inplace=True)\n",
    "df.to_csv(f'../../data/ml_datasets/oversampling/model_selection/{USER}_knn_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier()\n",
    "\n",
    "param_dist = {\"max_depth\": [5, 10, 20, None],\n",
    "              \"max_features\": sp_randint(3, N_FEATURES + 1),\n",
    "              \"min_samples_split\": [20, 50, 100],\n",
    "              \"min_samples_leaf\": [10, 30, 50, 100],\n",
    "              \"criterion\": [\"entropy\", \"gini\"],\n",
    "              \"class_weight\":['balanced', None, {0: 0.8, 1: 0.2}, {0: 0.6, 1: 0.4}],\n",
    "              \"n_estimators\": [50, 100, 150]}\n",
    "\n",
    "n_iter_search = 50 # Total-Iteration: 100\n",
    "rand_search = RandomizedSearchCV(clf, param_distributions=param_dist, \n",
    "                            n_iter=n_iter_search, \n",
    "                            n_jobs=N_JOBS, \n",
    "                            scoring=scoring,\n",
    "                            refit=False,\n",
    "                            cv=ps)\n",
    "rand_search.fit(X_combined, y_combined);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rand_search.cv_results_)\n",
    "df.sort_values(by='rank_test_f1_macro', inplace=True)\n",
    "df.to_csv(f'../../data/ml_datasets/oversampling/model_selection/{USER}_random_forest_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = XGBClassifier()\n",
    "param_dist = {\n",
    "    #\"n_estimators\": [25, 50, 100], #Jacopo\n",
    "    \"n_estimators\": [250, 500],  # Simone\n",
    "    \"max_depth\": [2, 3, 4, 5],  \n",
    "    \"learning_rate\": [1, 0.1, 0.01, 0.001, 0.0001] \n",
    "}\n",
    "\n",
    "rand_search = GridSearchCV(clf, param_grid=param_dist,  \n",
    "                            n_jobs=N_JOBS, \n",
    "                            scoring=scoring,\n",
    "                            refit=False,\n",
    "                            cv=ps)\n",
    "rand_search.fit(X_combined, y_combined);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rand_search.cv_results_)\n",
    "df.sort_values(by='rank_test_f1_macro', inplace=True)\n",
    "df.to_csv(f'../../data/ml_datasets/oversampling/model_selection/{USER}_xgb_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = AdaBoostClassifier()\n",
    "param_dist = {\n",
    "    # \"n_estimators\": [25, 50, 100], #Jacopo  \n",
    "    \"n_estimators\": [250, 500], # Simone \n",
    "    \"learning_rate\": [1, 0.1, 0.01, 0.001, 0.0001],  \n",
    "    \"algorithm\": ['SAMME'] \n",
    "}\n",
    "\n",
    "rand_search = GridSearchCV(clf, param_grid=param_dist, \n",
    "                            n_jobs=N_JOBS, \n",
    "                            scoring=scoring,\n",
    "                            refit=False,\n",
    "                            cv=ps)\n",
    "rand_search.fit(X_combined, y_combined);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rand_search.cv_results_)\n",
    "df.sort_values(by='rank_test_f1_macro', inplace=True)\n",
    "df.to_csv(f'../../data/ml_datasets/oversampling/model_selection/{USER}_ada_boost_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyHyperModel(keras_tuner.HyperModel):\n",
    "    def build(self, hp, units, dropout_rate, learning_rate):\n",
    "        model = keras.Sequential()\n",
    "\n",
    "        model.add(keras.layers.Dense(\n",
    "            units,\n",
    "            activation='relu')),\n",
    "        model.add(keras.layers.Dropout(rate=dropout_rate))\n",
    "        model.add(keras.layers.Dense(\n",
    "            units//2,\n",
    "            activation='relu'))\n",
    "        model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "        # Configura l'ottimizzatore con il learning rate scelto\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        f1 = keras.metrics.F1Score(average='macro', threshold=0.5, name=\"f1_macro\", dtype=None)\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=[f1])\n",
    "  \n",
    "        return model\n",
    "    \n",
    "    def fit(self, hp, model, x, y, epochs, batch_size, **kwargs):\n",
    "        return model.fit(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            verbose=False,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with batch_size=512, epochs=10, units_layer1=128, drop_rate=0.2, learning_rate=0.0006812920690579615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1734175418.308142     692 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with batch_size=256, epochs=10, units_layer1=256, drop_rate=0.6000000000000001, learning_rate=0.0006812920690579615\n",
      "Training with batch_size=1024, epochs=20, units_layer1=256, drop_rate=0.0, learning_rate=0.001\n",
      "Training with batch_size=1024, epochs=20, units_layer1=256, drop_rate=0.0, learning_rate=0.0031622776601683794\n",
      "Training with batch_size=1024, epochs=10, units_layer1=64, drop_rate=0.6000000000000001, learning_rate=0.004641588833612777\n",
      "Training with batch_size=256, epochs=10, units_layer1=64, drop_rate=0.6000000000000001, learning_rate=0.001\n",
      "Training with batch_size=256, epochs=10, units_layer1=256, drop_rate=0.8, learning_rate=0.0031622776601683794\n",
      "Training with batch_size=512, epochs=20, units_layer1=64, drop_rate=0.6000000000000001, learning_rate=0.0014677992676220691\n",
      "Training with batch_size=512, epochs=10, units_layer1=128, drop_rate=0.6000000000000001, learning_rate=0.00046415888336127773\n",
      "Training with batch_size=256, epochs=10, units_layer1=256, drop_rate=0.8, learning_rate=0.00031622776601683794\n",
      "Training with batch_size=256, epochs=10, units_layer1=32, drop_rate=0.8, learning_rate=0.002154434690031882\n",
      "Training with batch_size=1024, epochs=30, units_layer1=256, drop_rate=0.6000000000000001, learning_rate=0.0014677992676220691\n",
      "Training with batch_size=1024, epochs=20, units_layer1=128, drop_rate=0.6000000000000001, learning_rate=0.0031622776601683794\n",
      "Training with batch_size=1024, epochs=30, units_layer1=64, drop_rate=0.4, learning_rate=0.00046415888336127773\n",
      "Training with batch_size=256, epochs=10, units_layer1=256, drop_rate=0.6000000000000001, learning_rate=0.01\n",
      "Training with batch_size=256, epochs=30, units_layer1=64, drop_rate=0.6000000000000001, learning_rate=0.01\n",
      "Training with batch_size=256, epochs=30, units_layer1=64, drop_rate=0.2, learning_rate=0.002154434690031882\n",
      "Training with batch_size=256, epochs=30, units_layer1=32, drop_rate=0.2, learning_rate=0.0014677992676220691\n",
      "Training with batch_size=256, epochs=20, units_layer1=64, drop_rate=0.0, learning_rate=0.0014677992676220691\n",
      "Training with batch_size=512, epochs=10, units_layer1=256, drop_rate=0.4, learning_rate=0.00031622776601683794\n",
      "Training with batch_size=256, epochs=20, units_layer1=128, drop_rate=0.0, learning_rate=0.00046415888336127773\n",
      "Training with batch_size=512, epochs=20, units_layer1=64, drop_rate=0.6000000000000001, learning_rate=0.0014677992676220691\n",
      "Training with batch_size=1024, epochs=30, units_layer1=64, drop_rate=0.4, learning_rate=0.006812920690579608\n",
      "Training with batch_size=1024, epochs=10, units_layer1=256, drop_rate=0.8, learning_rate=0.0006812920690579615\n",
      "Training with batch_size=1024, epochs=20, units_layer1=64, drop_rate=0.0, learning_rate=0.00046415888336127773\n",
      "Training with batch_size=1024, epochs=20, units_layer1=64, drop_rate=0.2, learning_rate=0.001\n",
      "Training with batch_size=256, epochs=30, units_layer1=256, drop_rate=0.8, learning_rate=0.0014677992676220691\n",
      "Training with batch_size=512, epochs=30, units_layer1=256, drop_rate=0.6000000000000001, learning_rate=0.0006812920690579615\n",
      "Training with batch_size=256, epochs=20, units_layer1=128, drop_rate=0.4, learning_rate=0.00031622776601683794\n",
      "Training with batch_size=512, epochs=20, units_layer1=256, drop_rate=0.8, learning_rate=0.004641588833612777\n",
      "Training with batch_size=1024, epochs=30, units_layer1=32, drop_rate=0.2, learning_rate=0.01\n",
      "Training with batch_size=256, epochs=10, units_layer1=128, drop_rate=0.2, learning_rate=0.00046415888336127773\n",
      "Training with batch_size=1024, epochs=30, units_layer1=128, drop_rate=0.4, learning_rate=0.0014677992676220691\n",
      "Training with batch_size=512, epochs=20, units_layer1=128, drop_rate=0.2, learning_rate=0.0031622776601683794\n",
      "Training with batch_size=1024, epochs=30, units_layer1=128, drop_rate=0.8, learning_rate=0.001\n",
      "Training with batch_size=512, epochs=20, units_layer1=32, drop_rate=0.6000000000000001, learning_rate=0.00031622776601683794\n",
      "Training with batch_size=256, epochs=20, units_layer1=64, drop_rate=0.4, learning_rate=0.006812920690579608\n",
      "Training with batch_size=256, epochs=30, units_layer1=128, drop_rate=0.6000000000000001, learning_rate=0.006812920690579608\n",
      "Training with batch_size=512, epochs=30, units_layer1=256, drop_rate=0.4, learning_rate=0.0006812920690579615\n",
      "Training with batch_size=512, epochs=20, units_layer1=128, drop_rate=0.8, learning_rate=0.01\n",
      "Training with batch_size=512, epochs=20, units_layer1=32, drop_rate=0.2, learning_rate=0.0014677992676220691\n",
      "Training with batch_size=512, epochs=10, units_layer1=64, drop_rate=0.4, learning_rate=0.00031622776601683794\n",
      "Training with batch_size=1024, epochs=30, units_layer1=64, drop_rate=0.6000000000000001, learning_rate=0.002154434690031882\n",
      "Training with batch_size=1024, epochs=30, units_layer1=64, drop_rate=0.4, learning_rate=0.0031622776601683794\n",
      "Training with batch_size=256, epochs=20, units_layer1=256, drop_rate=0.2, learning_rate=0.006812920690579608\n",
      "Training with batch_size=256, epochs=20, units_layer1=128, drop_rate=0.8, learning_rate=0.001\n",
      "Training with batch_size=1024, epochs=20, units_layer1=32, drop_rate=0.0, learning_rate=0.0031622776601683794\n",
      "Training with batch_size=1024, epochs=30, units_layer1=128, drop_rate=0.2, learning_rate=0.004641588833612777\n",
      "Training with batch_size=1024, epochs=20, units_layer1=64, drop_rate=0.0, learning_rate=0.0014677992676220691\n",
      "Training with batch_size=512, epochs=10, units_layer1=32, drop_rate=0.6000000000000001, learning_rate=0.004641588833612777\n"
     ]
    }
   ],
   "source": [
    "rounds = 50\n",
    "config_results = []\n",
    "\n",
    "for _ in range(rounds):\n",
    "    hp = HyperParameters()\n",
    "    hyper_ae = MyHyperModel()\n",
    "    batch_size = hp.Fixed(\"batch_size\", random.choice([256, 512, 1024])) \n",
    "    epochs = hp.Fixed(\"epochs\", random.choice([10, 20, 30])) \n",
    "    units_layer1 = hp.Fixed('units_layer1', random.choice([32, 64, 128, 256]))\n",
    "    drop_rate = hp.Fixed('rate', random.choice(np.arange(0., 0.9, 0.2))) \n",
    "    #learning_rate = hp.Fixed(\"learning_rate\", random.choice(np.logspace(-5, -3.5, num=10))) # Jacopo\n",
    "    learning_rate = hp.Fixed(\"learning_rate\", random.choice(np.logspace(-3.5, -2, num=10))) # Simone\n",
    "\n",
    "    print(f\"Training with batch_size={batch_size}, epochs={epochs}, units_layer1={units_layer1}, drop_rate={drop_rate}, learning_rate={learning_rate}\")\n",
    "\n",
    "    model = hyper_ae.build(hp, units_layer1, drop_rate, learning_rate)\n",
    "\n",
    "    y_val = val_label.reshape(-1, 1)\n",
    "    y_train = train_label.reshape(-1, 1)\n",
    "\n",
    "\n",
    "    metrics = hyper_ae.fit(hp, model, train_set, y_train, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "    val_out = model.predict(val_set, verbose=False)\n",
    "    val_out = (val_out >= 0.5).astype(int)\n",
    "    f1_0 = f1_class_0._score_func(y_val, val_out)\n",
    "    f1_1 = f1_class_1._score_func(y_val, val_out)\n",
    "    f1_macro = (f1_0 + f1_1)/2\n",
    "\n",
    "    config = {\n",
    "        \"batch_size\": batch_size,\n",
    "        \"epochs\": epochs,\n",
    "        \"units_layer1\": units_layer1,\n",
    "        \"units_layer2\": units_layer1//2,\n",
    "        \"drop_rate\": drop_rate,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"f1_0\": f1_0,\n",
    "        \"f1_1\": f1_1,\n",
    "    }\n",
    "\n",
    "    config_results.append(config)\n",
    "\n",
    "df = pd.DataFrame(config_results)\n",
    "df.sort_values(by='f1_macro', inplace=True, ascending=False)\n",
    "df.to_csv(f'../../data/ml_datasets/oversampling/model_selection/{USER}_nn_results.csv', index=False) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discretizing 9 features: [0, 1, 3, 4, 5, 6, 7, 8, 9]\n",
      "\n",
      "growing ruleset...\n",
      "initial model: []\n",
      "\n",
      "pos_growset 125797 pos_pruneset 103093\n",
      "neg_growset 125797 neg_pruneset 103093\n",
      "grew rule: [6=<0.29^4=0.27-0.38^7=<165.85^3=0.02-0.024^0=160.0-168.6^1=<19.54^11=0.0]\n",
      "pruned rule: [6=<0.29^4=0.27-0.38^7=<165.85^3=0.02-0.024^0=160.0-168.6^1=<19.54]\n",
      "updated ruleset: [[6=<0.29^4=0.27-0.38^7=<165.85^3=0.02-0.024^0=160.0-168.6^1=<19.54]]\n",
      "\n",
      "pos_growset 125726 pos_pruneset 103034\n",
      "neg_growset 125795 neg_pruneset 103091\n",
      "grew rule: [6=<0.29^4=>0.38^2=1.0^12=0.0^3=>0.024^7=<165.85]\n",
      "pruned rule: [6=<0.29^4=>0.38^2=1.0^12=0.0^3=>0.024]\n",
      "updated ruleset: [[6=<0.29^4=0.27-0.38^7=<165.85^3=0.02-0.024^0=160.0-168.6^1=<19.54] V [6=<0.29^4=>0.38^2=1.0^12=0.0^3=>0.024]]\n",
      "\n",
      "\n",
      "GREW INITIAL RULESET:\n",
      "[[6=<0.29 ^ 4=0.27-0.38 ^ 7=<165.85 ^ 3=0.02-0.024 ^ 0=160.0-168.6 ^ 1=<19.54] V\n",
      "[6=<0.29 ^ 4=>0.38 ^ 2=1.0 ^ 12=0.0 ^ 3=>0.024]]\n",
      "\n",
      "optimization run 1 of 3\n",
      "optimizing ruleset...\n",
      "\n",
      "grew rule: [6=<0.29^4=>0.38^2=1.0^5=0.151-0.244]\n",
      "grew rule: [6=<0.29^4=0.27-0.38^7=<165.85^3=0.02-0.024^0=160.0-168.6^1=<19.54^5=0.0656-0.077]\n",
      "\n",
      "rule 1 of 2\n",
      "original: [6=<0.29^4=0.27-0.38^7=<165.85^3=0.02-0.024^0=160.0-168.6^1=<19.54]\n",
      "replacement: [6=<0.29]\n",
      "revision: [6=<0.29]\n",
      "*best: [6=<0.29]\n",
      "\n",
      "grew rule: [6=<0.29^4=>0.38^7=<165.85^2=1.0^12=0.0^3=>0.024]\n",
      "grew rule: [6=<0.29^4=>0.38^2=1.0^12=0.0^3=>0.024^7=<165.85]\n",
      "\n",
      "rule 2 of 2\n",
      "original: [6=<0.29^4=>0.38^2=1.0^12=0.0^3=>0.024]\n",
      "replacement: [6=<0.29]\n",
      "revision: [6=<0.29]\n",
      "*best: [6=<0.29]\n",
      "best already included in optimization -- retaining original\n",
      "\n",
      "\n",
      "OPTIMIZED RULESET:\n",
      "iteration 1 of 3\n",
      " modified rules [0]\n",
      "[[6=<0.29] V\n",
      "[6=<0.29 ^ 4=>0.38 ^ 2=1.0 ^ 12=0.0 ^ 3=>0.024]]\n",
      "\n",
      "optimization run 2 of 3\n",
      "optimizing ruleset...\n",
      "\n",
      "grew rule: [6=<0.29^4=>0.38^7=<165.85^1=20.45-20.76^10=0.0]\n",
      "grew rule: [6=<0.29^4=>0.38^7=<165.85^1=20.45-20.76^10=0.0]\n",
      "\n",
      "rule 1 of 2\n",
      "original: [6=<0.29]\n",
      "replacement: unchanged\n",
      "revision: unchanged\n",
      "*best: unchanged\n",
      "best already included in optimization -- retaining original\n",
      "\n",
      "grew rule: [6=0.29-0.34^12=0.0^4=0.27-0.38^7=<165.85^11=1.0^9=0.47-0.61^5=0.114-0.128]\n",
      "grew rule: [6=<0.29^4=>0.38^2=1.0^12=0.0^3=>0.024]\n",
      "\n",
      "rule 2 of 2\n",
      "original: [6=<0.29^4=>0.38^2=1.0^12=0.0^3=>0.024]\n",
      "replacement: [6=0.29-0.34]\n",
      "revision: [6=<0.29]\n",
      "*best: [6=0.29-0.34]\n",
      "\n",
      "\n",
      "OPTIMIZED RULESET:\n",
      "iteration 2 of 3\n",
      " modified rules [1]\n",
      "[[6=<0.29] V\n",
      "[6=0.29-0.34]]\n",
      "\n",
      "optimization run 3 of 3\n",
      "optimizing ruleset...\n",
      "\n",
      "grew rule: [6=<0.29^4=0.27-0.38^11=1.0^7=<165.85^8=0.0028-0.0042^0=146.4-160.0]\n",
      "grew rule: [6=<0.29^4=0.27-0.38^11=1.0^7=<165.85^8=0.0028-0.0042^0=146.4-160.0]\n",
      "\n",
      "rule 1 of 2\n",
      "original: [6=<0.29]\n",
      "replacement: unchanged\n",
      "revision: unchanged\n",
      "*best: unchanged\n",
      "best already included in optimization -- retaining original\n",
      "\n",
      "grew rule: [6=0.29-0.34^12=0.0^4=0.27-0.38^7=<165.85^11=1.0^8=0.0042-0.0079^5=0.093-0.108]\n",
      "grew rule: [6=0.29-0.34^12=0.0^4=0.27-0.38^7=<165.85^11=1.0^8=0.0042-0.0079^5=0.093-0.108]\n",
      "\n",
      "rule 2 of 2\n",
      "original: [6=0.29-0.34]\n",
      "replacement: unchanged\n",
      "revision: unchanged\n",
      "*best: unchanged\n",
      "best already included in optimization -- retaining original\n",
      "\n",
      "\n",
      "OPTIMIZED RULESET:\n",
      "iteration 3 of 3\n",
      " modified rules []\n",
      "[[6=<0.29] V\n",
      "[6=0.29-0.34]]\n",
      "\n",
      "228890 pos left. Growing final rules...\n",
      "growing ruleset...\n",
      "initial model: [[6=<0.29] V [6=0.29-0.34]]\n",
      "\n",
      "pos_growset 125797 pos_pruneset 103093\n",
      "neg_growset 125797 neg_pruneset 103093\n",
      "grew rule: [6=<0.29^4=0.27-0.38^7=<165.85^3=0.02-0.024^2=0.0^12=0.0^0=160.0-168.6]\n",
      "pruned rule unchanged\n",
      "updated ruleset: ...[[6=0.29-0.34] V [6=<0.29^4=0.27-0.38^7=<165.85^3=0.02-0.024^2=0.0^12=0.0^0=160.0-168.6]]\n",
      "\n",
      "pos_growset 125755 pos_pruneset 103058\n",
      "neg_growset 125797 neg_pruneset 103093\n",
      "grew rule: [6=<0.29^4=>0.38^2=1.0^12=0.0^13=0.0^8=0.0042-0.0079]\n",
      "pruned rule unchanged\n",
      "updated ruleset: ...[[6=<0.29^4=0.27-0.38^7=<165.85^3=0.02-0.024^2=0.0^12=0.0^0=160.0-168.6] V [6=<0.29^4=>0.38^2=1.0^12=0.0^13=0.0^8=0.0042-0.0079]]\n",
      "\n",
      "GREW FINAL RULES\n",
      "[[6=<0.29] V\n",
      "[6=0.29-0.34] V\n",
      "[6=<0.29 ^ 4=0.27-0.38 ^ 7=<165.85 ^ 3=0.02-0.024 ^ 2=0.0 ^ 12=0.0 ^ 0=160.0-168.6] V\n",
      "[6=<0.29 ^ 4=>0.38 ^ 2=1.0 ^ 12=0.0 ^ 13=0.0 ^ 8=0.0042-0.0079]]\n",
      "\n",
      "Optimizing dl...\n",
      "FINAL RULESET:\n",
      "[[6=<0.29] V\n",
      "[6=0.29-0.34]]\n",
      "\n",
      "rule 0 [6=<0.29]\n",
      "n_samples: 457780 tp: 34175 fp: 10460 tpr: 0.7656547552369217 fpr: 0.2343452447630783 class_ns: (10460, 34175) class_freqs: (0.2343452447630783, 0.7656547552369217) smoothed_class_freqs: [0.2343571476577727, 0.7656428523422273]\n",
      "rule 1 [6=0.29-0.34]\n",
      "n_samples: 457780 tp: 33869 fp: 13274 tpr: 0.7184311562692234 fpr: 0.28156884373077656 class_ns: (13274, 33869) class_freqs: (0.28156884373077656, 0.7184311562692234) smoothed_class_freqs: [0.28157811008590516, 0.7184218899140948]\n",
      "tn: 205156 fn: 160846 tnr: 0.5605324561067972 fnr: 0.43946754389320275 ruleset_uncovered_class_ns: (205156, 160846) ruleset_uncovered_class_freqs (0.5605324561067972, 0.43946754389320275) smoothed_uncovered_class_freqs_ [0.5605321253319636, 0.43946787466803644]\n"
     ]
    }
   ],
   "source": [
    "param_dist = {\n",
    "    'prune_size': sp_uniform(0.1, 0.4),  # Distribuzione uniforme tra 0.1 e 0.5\n",
    "    'k': sp_randint(1, 11)               # Interi tra 1 e 10\n",
    "}\n",
    "\n",
    "#define the number of iters\n",
    "n_iter_search = 20\n",
    "#define the model\n",
    "clf = lw.RIPPER(\n",
    "    verbosity=2,         # Detailed logging for debugging\n",
    "    max_rules=10,        # Moderate rule complexity\n",
    "    max_rule_conds=7,    # Enough room for moderately complex conditions\n",
    "    max_total_conds=35   # Cap total conditions to avoid runaway complexity\n",
    ")\n",
    "#define the grid search\n",
    "rand_search = RandomizedSearchCV(estimator=clf, param_distributions=param_dist, n_iter=n_iter_search, \n",
    "                                 scoring=scoring, \n",
    "                                 refit=False, \n",
    "                                 n_jobs=N_JOBS,\n",
    "                                 cv=ps)\n",
    "#run the grid search\n",
    "rand_search.fit(X_combined, y_combined);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rand_search.cv_results_)\n",
    "df.sort_values(by='rank_test_f1_macro', inplace=True)\n",
    "df.to_csv(f'../../data/ml_datasets/oversampling/model_selection/{USER}_rule_based_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
