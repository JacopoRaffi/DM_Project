{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "### Data Mining Project 2024/25\n",
    "\n",
    "Authors: Nicola Emmolo, Simone Marzeddu, Jacopo Raffi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to find the best set of parameter setting, we can run a grid search\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import wittgenstein as lw\n",
    "import keras_tuner\n",
    "import keras\n",
    "from keras_tuner import HyperParameters\n",
    "import tensorflow as tf\n",
    "\n",
    "from statistics import mean, stdev\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "from scipy.stats import loguniform as sp_loguniform\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../../data/ml_datasets/oversampling/train_set.csv').sample(frac = 1, random_state=RANDOM_STATE) # shuffling the data so not to introduce bias\n",
    "val_data = pd.read_csv('../../data/ml_datasets/oversampling/val_set.csv')\n",
    "testing_data = pd.read_csv('../../data/ml_datasets/oversampling/test_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = train_data.pop('label')\n",
    "test_label = testing_data.pop('label')\n",
    "\n",
    "train_set = train_data\n",
    "train_set['race_season%autumn'] = train_set['race_season%autumn'].astype(int)\n",
    "train_set['race_season%spring'] = train_set['race_season%spring'].astype(int)\n",
    "train_set['race_season%summer'] = train_set['race_season%summer'].astype(int)\n",
    "train_set['race_season%winter'] = train_set['race_season%winter'].astype(int)\n",
    "\n",
    "val_set = val_data\n",
    "val_set['race_season%autumn'] = val_set['race_season%autumn'].astype(int)\n",
    "val_set['race_season%spring'] = val_set['race_season%spring'].astype(int)\n",
    "val_set['race_season%summer'] = val_set['race_season%summer'].astype(int)\n",
    "val_set['race_season%winter'] = val_set['race_season%winter'].astype(int)\n",
    "\n",
    "test_set = testing_data\n",
    "test_set['race_season%autumn'] = test_set['race_season%autumn'].astype(int)\n",
    "test_set['race_season%spring'] = test_set['race_season%spring'].astype(int)\n",
    "test_set['race_season%summer'] = test_set['race_season%summer'].astype(int)\n",
    "test_set['race_season%winter'] = test_set['race_season%winter'].astype(int)\n",
    "\n",
    "N_JOBS = 8\n",
    "USER = 'Jacopo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the parameters' values you want to try\n",
    "def f1_class_scorer(class_index):\n",
    "    def score_function(y_true, y_pred):\n",
    "        # Calcola F1 per ciascuna classe e ritorna quella specificata\n",
    "        return f1_score(y_true, y_pred, average=None)[class_index]\n",
    "    return make_scorer(score_function)\n",
    "\n",
    "# Scorer per la classe 0 e 1\n",
    "f1_class_0 = f1_class_scorer(0)  # Classe 0\n",
    "f1_class_1 = f1_class_scorer(1)  # Classe 1\n",
    "\n",
    "\n",
    "scoring={\n",
    "        'f1_macro': 'f1_macro',   # F1 macro per entrambe le classi\n",
    "        'f1_0': f1_class_0,  # F1 solo per classe 0\n",
    "        'f1_1': f1_class_1   # F1 solo per classe 1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\"max_depth\": [3, 5, 10, 15, 20, None],\n",
    "              \"max_features\": sp_randint(3, len(train_set.iloc[0]) + 1),\n",
    "              \"min_samples_split\": [20, 30, 50, 100],\n",
    "              \"min_samples_leaf\": [10, 20, 30, 50, 100],\n",
    "              \"criterion\": [\"entropy\", \"gini\"],\n",
    "              \"class_weight\":['balanced', None, {0: 0.8, 1: 0.2}, {0: 0.6, 1: 0.4}]} # class weights are related to over/undersampling chosen\n",
    "#define the number of iters\n",
    "n_iter_search = 200 # Total-Iteration: 400\n",
    "#define the model\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "#define the grid search\n",
    "rand_search = RandomizedSearchCV(clf, param_distributions=param_dist, \n",
    "                            n_iter=n_iter_search, \n",
    "                            n_jobs=N_JOBS, \n",
    "                            scoring=scoring,\n",
    "                            refit=False)\n",
    "#run the grid search\n",
    "rand_search.fit(train_set, train_label);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rand_search.cv_results_)\n",
    "df.sort_values(by='rank_test_f1_macro', inplace=True)\n",
    "df.to_csv(f'../../data/ml_datasets/oversampling/model_selection/{USER}_decision_tree_results.csv', index=False)\n",
    "df.head()[['mean_test_f1_macro', 'mean_test_f1_0', 'mean_test_f1_1', 'rank_test_f1_macro']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the parameters' values you want to try\n",
    "param_dist = {\"C\": sp_loguniform(1e-4, 1e2)}\n",
    "#define the number of iters\n",
    "n_iter_search = 50 # Total-Iteration: 100\n",
    "#define the model\n",
    "clf = LinearSVC()\n",
    "#define the grid search\n",
    "rand_search = RandomizedSearchCV(clf, param_distributions=param_dist, \n",
    "                            n_iter=n_iter_search, \n",
    "                            n_jobs=N_JOBS, \n",
    "                            scoring=scoring,\n",
    "                            refit=False)\n",
    "#run the grid search\n",
    "rand_search.fit(train_set, train_label);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rand_search.cv_results_)\n",
    "df.sort_values(by='rank_test_f1_macro', inplace=True)\n",
    "df.to_csv(f'../../data/ml_datasets/oversampling/model_selection/{USER}_svm_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the parameters' values you want to try\n",
    "scoring_metrics = {\n",
    "    'recall': 'recall',\n",
    "    'precision': 'precision',\n",
    "    'f1': 'f1'\n",
    "}\n",
    "\n",
    "param_dist = {}\n",
    "#define the number of iters\n",
    "n_iter_search = 1\n",
    "#define the model\n",
    "clf = GaussianNB()\n",
    "#define the grid search\n",
    "rand_search = RandomizedSearchCV(clf, param_distributions=param_dist, #CrossValidation per confrontabilit√†, non model selection\n",
    "                            n_iter=n_iter_search, \n",
    "                            n_jobs=1, \n",
    "                            scoring=scoring,\n",
    "                            refit=False)\n",
    "#run the grid search\n",
    "rand_search.fit(train_set, train_label);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rand_search.cv_results_)\n",
    "df.sort_values(by='rank_test_f1_macro', inplace=True)\n",
    "df.to_csv(f'../../data/ml_datasets/oversampling/model_selection/{USER}_naive_bayes_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rischiamo che il mapping degli attributi categorici ordinali (senza one-hot) crei problemi nel K-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {'n_neighbors': [5, 15, 25], # Jacopo\n",
    "              #'n_neighbors': [40, 50], # Simone\n",
    "              'algorithm': ['ball_tree', 'kd_tree', 'brute'],}\n",
    "\n",
    "tmp_train_set = train_set.drop(columns=['cyclist_age_group', 'race_season%autumn', 'race_season%spring', 'race_season%summer', 'race_season%winter'])\n",
    "#define the model\n",
    "clf = KNeighborsClassifier()\n",
    "#define the grid search\n",
    "rand_search = GridSearchCV(clf, param_grid=param_dist,\n",
    "                            n_jobs=N_JOBS, \n",
    "                            scoring=scoring,\n",
    "                            refit=False)\n",
    "#run the grid search\n",
    "rand_search.fit(tmp_train_set, train_label);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rand_search.cv_results_)\n",
    "df.sort_values(by='rank_test_f1_macro', inplace=True)\n",
    "df.to_csv(f'../../data/ml_datasets/oversampling/model_selection/{USER}_knn_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier()\n",
    "\n",
    "param_dist = {\"max_depth\": [5, 10, 20, None],\n",
    "              \"max_features\": sp_randint(3, len(train_set.iloc[0]) + 1),\n",
    "              \"min_samples_split\": [20, 50, 100],\n",
    "              \"min_samples_leaf\": [10, 30, 50, 100],\n",
    "              \"criterion\": [\"entropy\", \"gini\"],\n",
    "              \"class_weight\":['balanced', None, {0: 0.8, 1: 0.2}, {0: 0.6, 1: 0.4}],\n",
    "              \"n_estimators\": [50, 100, 150]}\n",
    "\n",
    "n_iter_search = 50 # Total-Iteration: 100\n",
    "rand_search = RandomizedSearchCV(clf, param_distributions=param_dist, \n",
    "                            n_iter=n_iter_search, \n",
    "                            n_jobs=N_JOBS, \n",
    "                            scoring=scoring,\n",
    "                            refit=False)\n",
    "rand_search.fit(train_set, train_label);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rand_search.cv_results_)\n",
    "df.sort_values(by='rank_test_f1_macro', inplace=True)\n",
    "df.to_csv(f'../../data/ml_datasets/oversampling/model_selection/{USER}_random_forest_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = XGBClassifier()\n",
    "param_dist = {\n",
    "    \"n_estimators\": [25, 50, 100], #Jacopo\n",
    "    # \"n_estimators\": [250, 500],  Simone\n",
    "    \"max_depth\": [2, 3, 4, 5],  \n",
    "    \"learning_rate\": [1, 0.1, 0.01, 0.001, 0.0001] \n",
    "}\n",
    "\n",
    "rand_search = GridSearchCV(clf, param_grid=param_dist,  \n",
    "                            n_jobs=N_JOBS, \n",
    "                            scoring=scoring,\n",
    "                            refit=False)\n",
    "rand_search.fit(train_set, train_label);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rand_search.cv_results_)\n",
    "df.sort_values(by='rank_test_f1_macro', inplace=True)\n",
    "df.to_csv(f'../../data/ml_datasets/oversampling/model_selection/{USER}_xgb_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = AdaBoostClassifier()\n",
    "param_dist = {\n",
    "    \"n_estimators\": [25, 50, 100], # Jacopo  \n",
    "    # \"n_estimators\": [250, 500], Simone \n",
    "    \"learning_rate\": [1, 0.1, 0.01, 0.001, 0.0001],  \n",
    "    \"algorithm\": ['SAMME'] \n",
    "}\n",
    "\n",
    "rand_search = GridSearchCV(clf, param_grid=param_dist, \n",
    "                            n_jobs=N_JOBS, \n",
    "                            scoring=scoring,\n",
    "                            refit=False)\n",
    "rand_search.fit(train_set, train_label);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rand_search.cv_results_)\n",
    "df.sort_values(by='rank_test_f1_macro', inplace=True)\n",
    "df.to_csv(f'../../data/ml_datasets/oversampling/model_selection/{USER}_ada_boost_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyHyperModel(keras_tuner.HyperModel):\n",
    "    def build(self, hp, units, dropout_rate, learning_rate):\n",
    "        model = keras.Sequential()\n",
    "\n",
    "        model.add(keras.layers.Dense(\n",
    "            units,\n",
    "            activation='relu')),\n",
    "        model.add(keras.layers.Dropout(rate=dropout_rate))\n",
    "        model.add(keras.layers.Dense(\n",
    "            units//2,\n",
    "            activation='relu'))\n",
    "        model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "        # Configura l'ottimizzatore con il learning rate scelto\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        f1 = keras.metrics.F1Score(average='macro', threshold=0.5, name=\"f1_macro\", dtype=None)\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=[f1])\n",
    "  \n",
    "        return model\n",
    "    \n",
    "    def fit(self, hp, model, x, y, validation_data, epochs, batch_size, **kwargs):\n",
    "        return model.fit(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            validation_data=validation_data,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            verbose=False,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametri di KFold\n",
    "dev_x = train_set.to_numpy()\n",
    "dev_y = train_label.to_numpy()\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=False)\n",
    "\n",
    "\n",
    "rounds = 50\n",
    "config_results = []\n",
    "\n",
    "for _ in range(rounds):\n",
    "    hp = HyperParameters()\n",
    "    hyper_ae = MyHyperModel()\n",
    "    batch_size = hp.Fixed(\"batch_size\", random.choice([256, 512, 1024])) \n",
    "    epochs = hp.Fixed(\"epochs\", random.choice([5, 10])) \n",
    "    units_layer1 = hp.Fixed('units_layer1', random.choice([32, 64, 128, 256]))  \n",
    "    drop_rate = hp.Fixed('rate', random.choice(np.arange(0., 0.9, 0.2))) \n",
    "    learning_rate = hp.Fixed(\"learning_rate\", random.choice(np.logspace(-5, -3.5, num=10))) # Jacopo\n",
    "    #learning_rate = hp.Fixed(\"learning_rate\", random.choice(np.logspace(-3.5, -2, num=10))) Simone\n",
    "\n",
    "    print(f\"Training with batch_size={batch_size}, epochs={epochs}, units_layer1={units_layer1}, drop_rate={drop_rate}, learning_rate={learning_rate}\")\n",
    "\n",
    "    model = hyper_ae.build(hp, units_layer1, drop_rate, learning_rate)\n",
    "    f1_macro_scores = []\n",
    "    f1_0_scores = []\n",
    "    f1_1_scores = []\n",
    "\n",
    "    for train_index, val_index in kf.split(dev_x, dev_y):\n",
    "        x_train, x_val = dev_x[train_index], dev_x[val_index]\n",
    "        y_train, y_val = dev_y[train_index], dev_y[val_index]\n",
    "        y_val = y_val.reshape(-1, 1)\n",
    "        y_train = y_train.reshape(-1, 1)\n",
    "        \n",
    "        # Adatta il modello con i dati di training e validazione \n",
    "        metrics = hyper_ae.fit(hp, model, x_train, y_train, validation_data=(x_val, y_val), epochs=epochs, batch_size=batch_size)\n",
    "        \n",
    "        val_out = model.predict(x_val, verbose=False)\n",
    "        val_out = (val_out >= 0.5).astype(int)\n",
    "        f1_0_scores.append(f1_class_0._score_func(y_val, val_out))\n",
    "        f1_1_scores.append(f1_class_1._score_func(y_val, val_out))\n",
    "        f1_macro_scores.append(metrics.history['val_f1_macro'][-1])\n",
    "    \n",
    "    mean_f1_macro, std_f1_macro = mean(f1_macro_scores), stdev(f1_macro_scores)\n",
    "    mean_f1_0, std_f1_0 = mean(f1_0_scores), stdev(f1_0_scores)\n",
    "    mean_f1_1, std_f1_1 = mean(f1_1_scores), stdev(f1_1_scores)\n",
    "    config = {\n",
    "        \"batch_size\": batch_size,\n",
    "        \"epochs\": epochs,\n",
    "        \"units_layer1\": units_layer1,\n",
    "        \"units_layer2\": units_layer1//2,\n",
    "        \"drop_rate\": drop_rate,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"mean_f1_macro\": mean_f1_macro,\n",
    "        \"std_f1_macro\": std_f1_macro,\n",
    "        \"mean_f1_0\": mean_f1_0,\n",
    "        \"std_f1_0\": std_f1_0,\n",
    "        \"mean_f1_1\": mean_f1_1,\n",
    "        \"std_f1_1\": std_f1_1\n",
    "    }\n",
    "\n",
    "    config_results.append(config)\n",
    "\n",
    "df = pd.DataFrame(config_results)\n",
    "df.sort_values(by='mean_f1_macro', inplace=True, ascending=False)\n",
    "df.to_csv(f'../../data/ml_datasets/oversampling/model_selection/{USER}_nn_results.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    'prune_size': sp_uniform(0.1, 0.4),  # Distribuzione uniforme tra 0.1 e 0.5\n",
    "    'k': sp_randint(1, 11)               # Interi tra 1 e 10\n",
    "}\n",
    "\n",
    "#define the number of iters\n",
    "n_iter_search = 10\n",
    "#define the model\n",
    "clf = lw.RIPPER()\n",
    "#define the grid search\n",
    "rand_search = RandomizedSearchCV(estimator=clf, param_distributions=param_dist, n_iter=n_iter_search, \n",
    "                                 scoring=scoring, \n",
    "                                 refit=False, \n",
    "                                 n_jobs=2)\n",
    "#run the grid search\n",
    "rand_search.fit(train_set, train_label);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rand_search.cv_results_)\n",
    "df.sort_values(by='rank_test_f1_macro', inplace=True)\n",
    "df.to_csv(f'../../data/ml_datasets/oversampling/model_selection/{USER}_rule_based_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
