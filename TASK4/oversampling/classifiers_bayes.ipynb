{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification: Bayesian Search\n",
    "\n",
    "### Data Mining Project 2024/25\n",
    "\n",
    "Authors: Nicola Emmolo, Simone Marzeddu, Jacopo Raffi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to find the best set of parameter setting, we can run a grid search\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import wittgenstein as lw\n",
    "import keras_tuner\n",
    "import keras\n",
    "from keras_tuner import HyperParameters\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from statistics import mean, stdev\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "from scipy.stats import loguniform as sp_loguniform\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge results from previous random search\n",
    "USER_1 = 'Jacopo'\n",
    "USER_2 = 'Simone'\n",
    "\n",
    "models = ['ada_boost', 'nn', 'xgb', 'naive_bayes', 'random_forest', 'decision_tree', 'svm', 'rule_based', 'knn']\n",
    "\n",
    "for model in models:\n",
    "    path_1 = f'../../data/ml_datasets/oversampling/model_selection/{USER_1}_{model}_results.csv'\n",
    "    path_2 = f'../../data/ml_datasets/oversampling/model_selection/{USER_2}_{model}_results.csv'\n",
    "\n",
    "    concatenate_path = f'../../data/ml_datasets/oversampling/model_selection/{model}_results.csv'\n",
    "\n",
    "    df1 = pd.read_csv(path_1)\n",
    "    df2 = pd.read_csv(path_2)\n",
    "\n",
    "    df2['mean_test_f1_macro'] = (df2['mean_test_f1_1'] + df2['mean_test_f1_0']) / 2\n",
    "    df2['mean_test_f1_macro'] = (df2['std_test_f1_1'] + df2['std_test_f1_0']) / 2\n",
    "\n",
    "    # Concatena le righe\n",
    "    df_concatenato = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "    # Salva il risultato in un nuovo CSV\n",
    "    df_concatenato.to_csv(concatenate_path, index=False)  #to concatenate the two files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_see = ['mean_test_f1_micro', 'std_test_f1_micro', 'mean_test_f1_1', 'std_test_f1_1', 'mean_test_f1_0', 'std_test_f1_0', 'mean_test_f1_macro', 'std_test_f1_macro']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('../../data/ml_datasets/oversampling/model_selection/ada_boost_results.csv')\n",
    "# df.sort_values(by='mean_test_f1_micro', ascending=False, inplace=True)\n",
    "# params= [col for col in df.columns if col.startswith(\"param_classifier__\")]\n",
    "# df.head(n=10)[columns_to_see+params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['ada_boost', 'nn', 'xgb', 'naive_bayes', 'random_forest', 'decision_tree', 'svm', 'rule_based', 'knn']\n",
    "\n",
    "df_results = pd.read_csv('../../data/ml_datasets/oversampling/model_selection/nn_results.csv')\n",
    "df_results = df_results.rename(columns={'mean_f1_micro': 'mean_test_f1_micro', \n",
    "                                        'std_f1_micro': 'std_test_f1_micro',\n",
    "                                        'mean_f1_1': 'mean_test_f1_1',\n",
    "                                        'std_f1_1': 'std_test_f1_1',\n",
    "                                        'mean_f1_0': 'mean_test_f1_0',\n",
    "                                        'std_f1_0': 'std_test_f1_0',\n",
    "                                        'mean_f1_macro': 'mean_test_f1_macro', \n",
    "                                        'std_f1_macro': 'std_test_f1_macro',})\n",
    "print(df_results.columns)\n",
    "df_results = df_results[columns_to_see]\n",
    "df_results['model'] = 'nn'\n",
    "models.remove('nn')\n",
    "\n",
    "columns_to_see = ['model'] + columns_to_see\n",
    "for model in models:\n",
    "    path = f'../../data/ml_datasets/oversampling/model_selection/{model}_results.csv'\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "    df['model'] = model\n",
    "    df['mean_test_f1_macro'] = (df['mean_test_f1_1'] + df['mean_test_f1_0']) / 2\n",
    "    df.sort_values(by='mean_test_f1_macro', ascending=False, inplace=True)\n",
    "    df = df.head(10)\n",
    "    df = df[columns_to_see + ['mean_test_f1_macro']]\n",
    "\n",
    "    df_results = pd.concat([df_results, df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.sort_values(by='mean_test_f1_macro', ascending=False, inplace=True)\n",
    "df_results.head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Winner models for oversampling:\n",
    "- Random Forests\n",
    "- XGB\n",
    "- Decision Tree\n",
    "- Rule-Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "\n",
    "train_data = pd.read_csv('../../data/ml_datasets/oversampling/train_set.csv').sample(frac = 1, random_state=RANDOM_STATE) # shuffling the data so not to introduce bias\n",
    "val_data = pd.read_csv('../../data/ml_datasets/oversampling/val_set.csv')\n",
    "testing_data = pd.read_csv('../../data/ml_datasets/oversampling/test_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = train_data.pop('label')\n",
    "val_label = val_data.pop('label')\n",
    "test_label = testing_data.pop('label')\n",
    "\n",
    "train_set = train_data\n",
    "train_set['race_season%autumn'] = train_set['race_season%autumn'].astype(int)\n",
    "train_set['race_season%spring'] = train_set['race_season%spring'].astype(int)\n",
    "train_set['race_season%summer'] = train_set['race_season%summer'].astype(int)\n",
    "train_set['race_season%winter'] = train_set['race_season%winter'].astype(int)\n",
    "\n",
    "val_set = val_data\n",
    "val_set['race_season%autumn'] = val_set['race_season%autumn'].astype(int)\n",
    "val_set['race_season%spring'] = val_set['race_season%spring'].astype(int)\n",
    "val_set['race_season%summer'] = val_set['race_season%summer'].astype(int)\n",
    "val_set['race_season%winter'] = val_set['race_season%winter'].astype(int)\n",
    "\n",
    "test_set = testing_data\n",
    "test_set['race_season%autumn'] = test_set['race_season%autumn'].astype(int)\n",
    "test_set['race_season%spring'] = test_set['race_season%spring'].astype(int)\n",
    "test_set['race_season%summer'] = test_set['race_season%summer'].astype(int)\n",
    "test_set['race_season%winter'] = test_set['race_season%winter'].astype(int)\n",
    "\n",
    "N_JOBS = 4\n",
    "USER = 'Jacopo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the parameters' values you want to try\n",
    "def f1_class_scorer(class_index):\n",
    "    def score_function(y_true, y_pred):\n",
    "        # Calcola F1 per ciascuna classe e ritorna quella specificata\n",
    "        return f1_score(y_true, y_pred, average=None)[class_index]\n",
    "    return make_scorer(score_function)\n",
    "\n",
    "# Scorer per la classe 0 e 1\n",
    "f1_class_0 = f1_class_scorer(0)  # Classe 0\n",
    "f1_class_1 = f1_class_scorer(1)  # Classe 1\n",
    "\n",
    "\n",
    "scoring={\n",
    "        'f1_macro': 'f1_macro',   # F1 macro per entrambe le classi\n",
    "        'f1_0': f1_class_0,  # F1 solo per classe 0\n",
    "        'f1_1': f1_class_1   # F1 solo per classe 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "def func(*args):\n",
    "    global i\n",
    "    print(f'Configurazione: {i}')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FEATURES = len(train_set.iloc[0])\n",
    "\n",
    "train_set = train_set.to_numpy()\n",
    "train_label = train_label.to_numpy()\n",
    "\n",
    "val_set = val_set.to_numpy()\n",
    "val_label = val_label.to_numpy()\n",
    "\n",
    "split_index = np.concatenate([\n",
    "    np.full(len(train_set), -1),  # -1 per training\n",
    "    np.zeros(len(val_set))   # 0 per validation\n",
    "])\n",
    "\n",
    "X_combined = np.vstack((train_set, val_set))\n",
    "y_combined = np.concatenate((train_label, val_label))\n",
    "\n",
    "ps = PredefinedSplit(test_fold=split_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree:\n",
    "- Class Weight: NaN\n",
    "- criterion: entropy, gini\n",
    "- max_depth: 8-12\n",
    "- max_features: 11 in su\n",
    "- min_samples_leaf: 5-70\n",
    "- min_samples_split: 10-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\"classifier__max_depth\": Integer(8, 20),\n",
    "              \"classifier__max_features\": Integer(11, N_FEATURES),\n",
    "              \"classifier__min_samples_split\": Integer(10, 50),\n",
    "              \"classifier__min_samples_leaf\": Integer(5, 70),\n",
    "              \"classifier__criterion\": Categorical(['gini', 'entropy'])}\n",
    "#define the number of iters\n",
    "n_iter_search = 100\n",
    "#define the model\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "#define the grid search\n",
    "rand_search = BayesSearchCV(clf, search_spaces=param_dist, \n",
    "                            n_iter=n_iter_search, \n",
    "                            n_jobs=N_JOBS, \n",
    "                            scoring=scoring,\n",
    "                            refit='f1_macro',\n",
    "                            cv=ps)\n",
    "#run the grid search\n",
    "rand_search.fit(X_combined, y_combined, callback=func);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rand_search.cv_results_)\n",
    "df.sort_values(by='rank_test_f1_macro', inplace=True)\n",
    "df.to_csv(f'../../data/ml_datasets/oversampling/model_selection/{USER}_decision_tree_results_bayes.csv', index=False)\n",
    "df.head(n=10)[['mean_test_f1_macro', 'std_test_f1_macro', 'mean_test_f1_1', 'std_test_f1_1', 'mean_test_f1_0', 'std_test_f1_0']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
